{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfbb6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83b275",
   "metadata": {},
   "source": [
    "# <center> Matrix methods and physics - Part II </center>\n",
    "\n",
    "## <center> The complexity of eigenvalue calculations </center>\n",
    "\n",
    "The calculation of matrix inverses closely matches how humans calculate matrix inverses. Gaussian elimination forms a central role in any algorithm trying to invert a nonsingular matrix.\n",
    "\n",
    "The calculation of eigenvalues is a different story. On pen-and-paper, the central idea of calculating the eigenvalues of a matrix is trying to find the zeros of a characteristic polynomial, which is found by calculating the determinant of a related matrix. Because we need to calculate the determinant, this method is not scalable at all!\n",
    "\n",
    "Notice that in calculating the determinant of a $3 \\times 3$ matrix, one can break it down to a sum of 3 $2 \\times 2$ determinants. Thus, there are $3!$ arithmetic calculations all-in-all. For a $4\\times4$ matrix, the same argument tells us that the complexity of the calculation rises up to $4!$.\n",
    "\n",
    "Thus, naively, calculating the eigenvalues of an $n \\times n$ matrix via a characteristic polynomial includes a calculation with a dizzying complexity of $O(n!)$. After which, one still needs to find the roots of the resulting characteristic polynomial. Compare this to the much more mundane complexity of matrix inverses: Gaussian elimination finishes in around $O(n^3)$ steps, for the same size of matrix.\n",
    "\n",
    "The books introduce two types of decompositions for matrix inverses and eigenvalue calcuations; respectively they are LU decomposition and QR decomposition. Both decompositions are calculated after $O(n^3)$ arithmetic operations. However, for the $QR$ decomposition to yield eigenvalues, you are asked to repeat or iterate the QR decomposition until a related matrix is approximately diagonal (or, all off-diagonal matrix elements are deemed numerically zero).\n",
    "\n",
    "How fast does this iteration converge to an approximately diagonal form? It depends greatly on the initial guess (given in the books as an identity matrix) and the distribution of the eigenvalues on the complex plane. In general, the iteration does not converge in a constant number of steps! The complexity $O(n^3)$ of each iteration of the QR decomposition may be misleading; it may take $n$-repetitions of the QR decomposition, $n^2$-repetitions or some badly behaved function of $f(n)$-repetitions.\n",
    "\n",
    "The modern way of calculating eigenvalues rest upon having a very good initial guess. The $O(n^3)$ complexity of each iteration of the QR algorithm assumes that the matrix being iterated upon is the most general. This may be reduced greatly (to $O(n^2)$) when the initial guess has a specific form (called a Hessenberg matrix).\n",
    "\n",
    "Also, by choosing an initial guess supposedly close to when the QR algorithm terminates, we reduce the number of times the QR algorithm is to be iterated.\n",
    "\n",
    "### <center> Krylov subspaces </center>\n",
    "\n",
    "To fully elucidate the details of how this initial guess is calculated would take several lecture notes. I will only handwave the details here.\n",
    "\n",
    "The main idea is this: the calculation of the eigenvectors of a matrix $M$ is closely related to when the QR algorithm terminates. Notice that if we have a matrix $V$ whose columns are the eigenvectors of $M$,\n",
    "\\begin{equation}\n",
    "M v_i = \\lambda _i v_i, \\qquad V = [v_1, v_2, \\dots, v_n]\n",
    "\\end{equation}\n",
    "then\n",
    "\\begin{equation}\n",
    "M V = A(\\lambda_1, \\lambda_2, \\dots, \\lambda_n) V\n",
    "\\end{equation}\n",
    "where $A(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$ is a diagonal matrix whose diagonal elements are given in order. Notice that $A$ and $V$ are both the end-points of the QR algorithm. One can think of the QR algorithm as a Newton-Raphson method on the above equation for the matrix $V$.\n",
    "\n",
    "In the same way that the Newton-Raphson method settles to a zero as quickly as possible whenever you start really close to one,  it seems that to reach this end-point as quickly as possible one needs a way to approximate the set of eigenvectors. One such approximate set of eigenvectors are called Krylov subspaces. The calculation of these Krylov subspaces, and their reduction to an appropriate Hessenberg form, are the subject of such algorithms like the Arnoldi iteration and the Lanczos algorithm.\n",
    "\n",
    "Recall that the number of iterations the QR algorithm depended on were two things: the initial guess for $V$ and the distribution of the eigenvalues on the complex plane. The number of iterations the QR algorithm needs to be repeated is still unknown; however we have mitigated the complexity as much as we can with the use of Krylov subspaces, by removing the dependence on the initial guess as much as possible. Without relying on special properties of matrices (and thus reducing the generality of our eigenvalue method), this is unfortunately the best that we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d91c7",
   "metadata": {},
   "source": [
    "## <center> Spectral and Pseudospectral methods </center>\n",
    "\n",
    "In Part II, we shall introduce another family of methods to solve differential equations: spectral and pseudospectral methods. These family of methods qualitatively differ from finite difference method from the fact that the derivatives are exactly satisfied. This is because we started with a complete function basis, say $\\phi$\n",
    "\\begin{equation}\n",
    "\\phi_n(x) = \\sin \\left( \\dfrac{n \\pi x}{L} \\right), \\qquad n = 1, 2, \\dots\n",
    "\\end{equation}\n",
    "and expanded the solution $\\psi(x)$ using this basis set\n",
    "\\begin{equation}\n",
    "\\psi(x) = \\sum_{n=1}^\\infty c_n \\phi_n(x),\n",
    "\\end{equation}\n",
    "Thus, when we insert this expression into the differential equation,\n",
    "\\begin{equation}\n",
    "\\hat{H} \\psi(x) = E \\psi(x),\n",
    "\\end{equation}\n",
    "we may compute the derivatives exactly. The derivatives of sine's and cosine's are simply other sine's and cosine's. No approximations there.\n",
    "\n",
    "The approximation comes in when we terminate the sum to some finite number of terms $N$. In this way, we go from a solution space that is complete and infinite dimensional (and thus uncomputable) to a solution space that is finite (and thus computable) but incomplete,\n",
    "\\begin{equation}\n",
    "\\psi(x) \\approx \\sum_{n=1}^N c_n \\phi_n(x),\n",
    "\\end{equation}\n",
    "\n",
    "All that remains now is to find a way to generate a algebraic set of equations on the coefficients $c_n$ which approximates the differential equation,\n",
    "\\begin{equation}\n",
    "\\hat{H} \\psi(x) = E \\psi(x)\n",
    "\\end{equation}\n",
    "There are two ways to generate these algebraic equations. The first family are called **spectral methods**: choose another finite subset of a complete function basis, say $\\gamma$. These functions are usually called our **test functions**. A set of algebraic of equations on $c_n$ is generated by taking each test function and multiplying the differential equation,\n",
    "\\begin{equation}\n",
    "\\gamma_m(x) \\hat{H} \\psi(x) = \\gamma_m(x) E \\psi(x)\n",
    "\\end{equation}\n",
    "and then integrating over the domain of the solution\n",
    "\\begin{equation}\n",
    "\\int_{0}^L \\gamma_m(x) \\hat{H} \\psi(x) dx = \\int_0^L \\gamma_m(x) E \\psi(x) dx. \n",
    "\\end{equation}\n",
    "By integrating, we remove the dependence of the differential equation on the set of basis functions $\\phi$ and $\\gamma$. Each test function generates one algebraic equation, so if we have $N$ test functions we are left with $N$ algebraic equations that generally fully specify the coefficients $c_n$.\n",
    "\n",
    "Some details:\n",
    "1. Whenever the basis functions $\\phi$ automatically satisfy the boundary conditions, these methods are called **Galerkin methods**. In the case of the infinite square well, recall that the wavefunctions necessarily vanish at the walls of the infinite square-well potential. Thus, the method in the books are Galerkin.\n",
    "2. Whenever the basis functions $\\phi$ do not satisfy the boundary conditions, these methods are called **Lanczos methods**. An additional set of algebraic equations must be supplied so that the boundary conditions are satisfied.\n",
    "3. It is usually the case that $\\phi = \\gamma$. This is what is used in the books. Whenever the test functions differ from the basis of the solution $\\psi(x)$, these methods are called **Petrov-Galerkin methods**. One may, for example, use a set of polynomials for $\\gamma$ instead of $\\sin$'s\n",
    "\n",
    "The second family are called **pseudospectral methods**. These methods generate a set of algebraic equations on $c_n$ by imposing that\n",
    "\\begin{equation}\n",
    "\\hat{H} \\psi(x) = E \\psi(x)\n",
    "\\end{equation}\n",
    "is satisfied on a discrete set of points,\n",
    "\\begin{equation}\n",
    "\\hat{H} \\psi(x_m) = E \\psi(x_m), \\qquad m = 1, 2, \\dots\n",
    "\\end{equation}\n",
    "By evaluating the basis functions on a mesh, we remove the dependence on the basis functions. Each point on the mesh generates one algebraic equation, so if we define an $N$-point mesh, we fully specify the coefficients $c_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e08a0",
   "metadata": {},
   "source": [
    "## <center> Eigenvalue problems, pseudospectral methods and physics </center>\n",
    "\n",
    "We shall now implement this second idea. Consider the Hamiltonian operator acting on the approximation for $\\psi(x)$,\n",
    "\\begin{equation}\n",
    "    \\hat{H}\\psi(x) = \\sum_{n=1}^N c_n \\hat{H}\\phi_n(x) = \\sum_{n=1}^N c_n \\left(- \\dfrac{\\hbar^2}{2m} \\dfrac{d^2}{dx^2} + V(x) \\right) \\sin \\left(\\dfrac{n \\pi x}{L} \\right) = \\sum_{n=1}^N c_n \\left( \\left(\\dfrac{n \\pi \\hbar}{L} \\right)^2 \\dfrac{1}{2m} + V(x) \\right) \\sin \\left(\\dfrac{n \\pi x}{L} \\right)\n",
    "\\end{equation}\n",
    "If we define an equally spaced $N$-point mesh spanning $x = [0,L]$,\n",
    "\\begin{equation}\n",
    "x_m \\in [0,L] \\qquad m = 1, 2, \\dots, N, \\qquad x_1 = 0, x_N = L, \\qquad (\\forall i = 1, 2, \\dots N-1):  x_{i+1} - x_{i} = h\n",
    "\\end{equation}\n",
    "then we can convert the above expression $\\hat{H}\\psi(x)$ into a matrix product between $H_{mn}$ and $c_n$ simply by evaluating on points on the mesh\n",
    "\\begin{equation}\n",
    "\\hat{H}\\psi(x) \\qquad \\to \\qquad H c\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "H_{mn} = \\left( \\left(\\dfrac{n \\pi \\hbar}{L} \\right)^2 \\dfrac{1}{2m} + V(x_m) \\right) \\sin \\left(\\dfrac{n \\pi x_m}{L} \\right)\n",
    "\\end{equation}\n",
    "We may also convert $E \\psi(x)$ into a matrix product $E M_{mn} c_n$,\n",
    "\\begin{equation}\n",
    "E \\psi(x) \\qquad \\to \\qquad E M c,\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "M_{mn} = \\sin \\left(\\dfrac{n \\pi x_m}{L} \\right).\n",
    "\\end{equation}\n",
    "Thus, the differential eigenvalue problem\n",
    "\\begin{equation}\n",
    "\\hat{H} \\psi(x) = E\\psi(x)\n",
    "\\end{equation}\n",
    "reduces to a matrix eigenvalue problem\n",
    "\\begin{equation}\n",
    "H c = E M c\n",
    "\\end{equation}\n",
    "There are several points to be discussed here. First, this is a generalized eigenvalue problem, since $M_{mn}$ is not an identity matrix. There are methods that solves the generalized eigenvalue problem available in scipy. There are several issues one can encounter. Unlike the usual eigenvalue problem, the generalized eigenvalue problem may still be well-defined even when one or both of $H_{mn}$ and $M_{mn}$ is singular (or non-invertible).\n",
    "\n",
    "Second, the indices $m = 1,N$ do not contribute to solving the coefficients $c_n$ at all. This is because the basis functions vanish at the boundaries, and thus the algebraic equation reduces to a tautological statement,\n",
    "\\begin{equation}\n",
    "0 = 0\n",
    "\\end{equation}\n",
    "Thus, we choose a modified mesh, where we generate $\\tilde{N} = N+2$ equally spaced points on the interval $[0,L]$ and remove the two boundary points.\n",
    "\n",
    "Finally, we may do away with the complexity of solving a generalized eigenvalue problem, since $M_{mn}$ should be non-singular. We leave this as a conjecture for now. However, this conjecture is related to the fact that $M_{mn}$ is constructed by evaluating the Fourier basis on an equally spaced mesh, and thus related to a discrete Fourier transform.\n",
    "\n",
    "Thus a 'regular' eigenvalue problem arises,\n",
    "\\begin{equation}\n",
    "A c = E c, \\qquad A = M^{-1} H\n",
    "\\end{equation}\n",
    "\n",
    "For simplicity, let us choose units such that\n",
    "\\begin{equation}\n",
    "\\dfrac{\\hbar^2}{2m} = 1\n",
    "\\end{equation}\n",
    "\n",
    "In which case the free particle in a box has the following eigenenergies,\n",
    "\\begin{equation}\n",
    "E_n = \\dfrac{n^2 \\pi^2}{L^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6eb580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partboxpert(V, L, N, vec = False):\n",
    "    \"\"\"\n",
    "    Calculates the eigenvalues of a perturbed particle in a box of length L, using a pseudo-spectral method\n",
    "    on a Fourier basis with N eigenfunctions\n",
    "    \"\"\"\n",
    "    x_eval = np.linspace(0,L,N+2)[1:-1]\n",
    "    V_eval = V(x_eval)\n",
    "    \n",
    "    ## generate M\n",
    "    M = np.zeros((N,N))\n",
    "    for m in range(N):\n",
    "        for n in range(N):\n",
    "            M[m,n] = np.sin((n+1) * np.pi * x_eval[m]/L)\n",
    "    \n",
    "    ## generate H\n",
    "    H = np.zeros((N,N))\n",
    "    for m in range(N):\n",
    "        for n in range(N):\n",
    "            H[m,n] = (((n+1) * np.pi/L)**2 + V_eval[m])*M[m,n]\n",
    "            \n",
    "    ## calculate A\n",
    "    A = np.dot(la.inv(M),H)\n",
    "    \n",
    "    ## calculate eigensystem\n",
    "    E, v = la.eig(A)\n",
    "    \n",
    "    if vec:\n",
    "        return E,v\n",
    "    else:\n",
    "        return E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4906d95",
   "metadata": {},
   "source": [
    "As a test, if we choose $L = \\pi$, then the eigenenergies are simply squares of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab4db36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1., 400., 361., 324., 289., 256., 225., 196., 169., 144., 121.,\n",
       "       100.,  81.,  64.,  49.,   4.,   9.,  36.,  25.,  16.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def V(x):\n",
    "    return 0*x\n",
    "\n",
    "L = np.pi\n",
    "N = 20\n",
    "\n",
    "np.real(partboxpert(V, L, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7ae09",
   "metadata": {},
   "source": [
    "Another simple test we can do, this time with a perturbation, is that if we raise the floor of the infinite square well by 1, then we should expect the eigenenergies to be homogeneously shifted upwards by 1 also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5736c7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2., 401., 362., 325., 290., 257., 226., 197., 170., 145., 122.,\n",
       "       101.,  82.,  65.,  50.,   5.,  10.,  37.,  26.,  17.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def V(x):\n",
    "    return 0*x + 1\n",
    "\n",
    "L = np.pi\n",
    "N = 20\n",
    "\n",
    "np.real(partboxpert(V, L, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10da545",
   "metadata": {},
   "source": [
    "To prove that this method is also correct for more general forms of the perturbing potential would need the use of perturbation theory. You may instead use the spectral method described in the book to determine if the following solution is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bbb5ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([401.57902576, 362.57271392, 325.57185494, 290.57169388,\n",
       "       257.57169591, 226.57176482, 197.57187699, 170.57202971,\n",
       "       145.57222937,   2.46590452,   5.60075353,  10.58972337,\n",
       "        17.58242895,  26.57852689,  37.57627323, 122.57248968,\n",
       "       101.57283337,  50.57486959,  65.57394106,  82.57329704])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def V(x):\n",
    "    return a x/L\n",
    "\n",
    "L = np.pi\n",
    "N = 20\n",
    "\n",
    "np.real(partboxpert(V, L, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3044e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([402.87467718, 364.18550733, 327.24578965, 292.26806638,\n",
       "       259.27906342, 228.28561741, 199.29015827, 172.2937609 ,\n",
       "       147.29700301, 124.30027296, 103.30391159,   3.00597714,\n",
       "         7.20099902,  12.39516728,  19.37671813,  28.35067077,\n",
       "        84.30830746,  39.33325412,  52.321837  ,  67.31399978])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def V(x):\n",
    "    return x**2\n",
    "\n",
    "L = np.pi\n",
    "N = 20\n",
    "\n",
    "np.real(partboxpert(V, L, N))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
